# Deciphering Big Data

---

## Module Overview

The "Deciphering Big Data" module focuses on developing the skills necessary to manage and analyse large datasets, including identifying and mitigating challenges, risks, and security issues associated with data wrangling. It also covers designing and developing solutions using appropriate methodologies, tools, and techniques for processing datasets in various environments.

---

## Learning Outcomes

1. Identify and manage challenges, security issues, and risks in data wrangling.
2. Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques.
3. Design, develop, and evaluate solutions for processing datasets and solving complex problems using relevant programming paradigms.
4. Develop and implement the skills required to be an effective member of a development team in a virtual professional environment.

---

## Key Artefacts

---

### Collaborative Discussion Forum Summaries

- [Collaborative Discussion 1 - The Data Collection Process](./Collaborative_Discussions/Collaborative_Discussion_1)
- [Collaborative Discussion 2 - Comparing Compliance Laws](./Collaborative_Discussions/Collaborative_Discussion_2)

---

### Development Team Project

#### Overview

This team project involved designing a logical database for Transport for London (TfL). Our goal was to create a robust and efficient database solution to manage large volumes of transport data. The project required collaboration on database design, data management pipeline creation, and critical evaluation of the selected database management system (DBMS). The team's task was to deliver a comprehensive report that met the client's needs, adhered to best practices in database design, and demonstrated an understanding of data management principles.

#### My Role and Contributions

- **Communication and Coordination:** Initiated communication through WhatsApp and forum threads to ensure clear, consistent updates and collaboration. This approach helped prevent miscommunications and ensured all team members were aligned on the project objectives and deadlines.
- **Task Assignment and Planning:** Created a detailed project plan and assigned tasks based on team members' strengths and availability. This strategy optimised workflow and helped meet deadlines efficiently.
- **Data Management Pipeline Development:** Led the design and implementation of a data management pipeline using Python. This included:
  - **Data Retrieval:** Developed Python scripts to access TfL's API, retrieve data in various formats (JSON, ZIP), and save it locally for further processing.
  - **Data Cleaning:** Implemented data cleaning techniques, such as handling missing values and removing duplicates, using Python libraries (e.g., `pandas`).
  - **Data Preparation for Database Loading:** Structured the cleaned data to be compatible with the chosen DBMS format, including normalising data to reduce redundancy.

*[Examples of my work](/.)*

#### Feedback

#### Feedback Received:
- **Strengths:**
  - The report demonstrated a good understanding of module topics and practical application.
  - Effective descriptions of data availability, user requirements, and a structured report format.
- **Areas for Improvement:**
  - More depth was needed in describing the logical design, particularly the ER diagrams.
  - The data management pipeline section required further elaboration and a clearer focus on essential elements.

Here is the [Initial Logical Design Report (Download)](./Team_Exercises/initial-report.docx)

#### Evaluation of Final Project vs. Initial Proposal

The final project significantly expanded upon the initial proposal, providing greater detail and addressing practical challenges more comprehensively. Key differences include:

API integration: The final report described the full process of capturing, cleaning, and normalising data from TfL’s API, refining the initial concept.
Enhanced logical design: The final project improved the relational integrity, with deeper focus on primary/foreign keys and achieving normalisation to 3NF.
DBMS evaluation: The final submission provided a thorough evaluation of PostgreSQL’s suitability, which was only briefly mentioned in the initial proposal.
In conclusion, the final project offered a more complete, scalable, and secure solution, building on the foundations of the initial proposal while addressing practical data management needs more effectively.

Here is the [Final Project Report (Download)](./Individual_Work/final-report.docx)

---

### Meeting Notes

A summary of notes made during meetings with my peers:

- Discussed skills to assess and allocate tasks accordingly.
- Agreed on the project goal and selected Transport for London as the target organisation for the database system.
- Set deadlines for tasks to ensure productive meetings and review progress effectively.
- Documented meeting minutes and task outcomes in the collaborative discussion forum on the student portal.
- Explained the primary and foreign keys for the database and determined how to split the data.
- Listened to my peers discuss their findings and contributions

---

### Individual Work

An overview of individual tasks completed, including:

- [Data Cleaning](./Individual_Work/data_cleaning/README.md)
- [Normalisation](./Individual_Work/Normalisation/README.md)
- [Data Build](./Individual_Work/Data_Build/README.md)
- [API Security Requirements](./Individual_Work/API_Security_Requirements/README.md)


---

## Professional Development Plan

---

### My Reflections

Key points from my [Full Reflective Piece](./Professional_Development/reflection.md):

- Peer feedback deepened my understanding of IoT privacy/security.
- Tackled data cleaning challenges, improving problem-solving.
- Collaboration enhanced project outcomes and learning.
- Adopted a security-first approach in API work.
- Led team efforts, improving communication and leadership.

---

### Professional Skills Gained

I have gained or enhanced the following skills:

- **Time Management:** Managing the TfL database project and discussions improved my ability to prioritise tasks and meet deadlines.
- **Commercial Awareness:** Case studies like IoT in smart cities deepened my understanding of data's commercial applications and long-term benefits.
- **Critical Thinking:** Processing and cleaning complex datasets, such as the Unicef dataset, honed my ability to critically evaluate challenges and solutions.
- **Communication:** Leading team discussions and writing reports improved my ability to explain technical concepts to diverse audiences.
- **IT and Digital Skills:** Implementing data pipelines and integrating APIs enhanced my technical proficiency in Python and data science tools.
- **Numeracy:** Handling large datasets and normalising values improved my numerical analysis skills.
- **Research Skills:** Exploring data privacy laws, like GDPR, sharpened my understanding of legal and ethical data handling requirements.
- **Interpersonal Skills:** Coordinating team communication strengthened my collaboration and teamwork abilities.
- **Problem-Solving:** Overcoming challenges in API data retrieval and database design developed my problem-solving capabilities.
- **Ethical Awareness:** Discussions on data privacy and anonymisation heightened my awareness of ethical issues.
- **Teamwork and Leadership:** Leading the TfL project and ensuring task alignment enhanced my leadership and teamwork.
- **Critical Reflection:** Peer feedback during discussions prompted me to reflect and refine my approach to technical challenges.

These skills are vital to my professional growth and will serve as a foundation for future roles in data science.

---

### Action Plan

To build on the skills developed during the **Deciphering Big Data** module, my action plan focuses on advancing my technical and strategic capabilities:

- **Deepen API Security Expertise**: I will further explore API security, particularly focusing on advanced topics like token management, encryption protocols, and authentication mechanisms through resources like OWASP.
- **Enhance Database Optimisation**: To improve performance with larger datasets, I will study indexing strategies and best practices for database optimisation, ensuring scalability and efficiency in future projects.
- **Stay Updated on Data Regulations**: I will continue to monitor evolving data protection regulations, especially GDPR and ICO guidelines, to stay informed about the latest compliance requirements for data security and privacy.
- **Develop Machine Learning Skills**: Expanding my knowledge in machine learning will allow me to handle more complex data problems and contribute to predictive analytics tasks, which are key in data science.
- **Improve Team Leadership**: Building on the leadership experience gained during the TfL project, I will seek opportunities to lead small projects and continue developing my ability to manage teams in a professional setting.
